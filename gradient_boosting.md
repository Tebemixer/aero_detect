**Градиентный бустинг**

**Градиентный бустинг** — это мощный метод машинного обучения, используемый для задач классификации. Он основан на идее последовательного построения ансамбля слабых моделей (чаще всего деревьев решений), где каждая следующая модель корректирует ошибки предыдущих. 

---

### **Бустинг**

**Бустинг** - последовательная процедура построения композиции слабых классификаторов, где каждый следующий алгоритм компенсирует ошибки предыдущих.

Бустинг является жадным алгоритмом, направленным на минимизацию ошибки на каждом шаге. Цель - объединенить слабые классификаторы в сильный классификатор.

Основные особенности бустинга:
   - Высокая **обобщающая** способность: качество на тестовой выборке может улучшаться даже после достижения нулевой ошибки на обучающих данных
   - **Гибкость** и адаптивность: работа с разными типами данных, разными моделями
   - **Универсальность** в различных задачах: применяется в задачах классификации, регрессии, ранжирования

### **Градиентный спуск в пространстве функций**:

- **Функция потерь**: $ L(y, a(x)) $, где $ a(x) = \sum_{k=0}^K \eta \cdot b_k(x) $ — ансамбль из $ K $ моделей с lerning rate $ \eta $.
- **Градиенты**: На шаге $ k $ вычисляется градиент по текущим предсказаниям:
  $$
  s_{i}^{k} = -\frac{\partial L(y_i, z)}{\partial z} \bigg|_{z=a_{k}}
  $$
  Новая модель $ b_{k+1}(x) $ обучается приближать эти остатки $ s_{i}^{k} $.
- **Обновление ансамбля**:
  $$
  a_{k+1}(x) = a_{k}(x) + \eta \cdot b_k(x)
  $$


[//]: # (source: deepseek +  https://education.yandex.ru/handbook/ml/article/gradientnyj-busting)

---

### **Алгоритм градиентного бустинга**
1. **Инициализация**:
   - Начальное предсказание минимизирует потери на обучающей выборке $ b_0(x) = \mathop {argmin}\limits_{b \in {\rm B}} \sum_{i=1}^n L(y_i, b\left({{x_i}} \right)) $ 
2. **Шаг алгоритма**:
     - Вычисление градиентов $ s_{i}^{k} $ для каждого объекта
     - Обучение модели $ b_k(x) $ на данных $ x_i $: $ b_k(x) = \mathop {argmin }\limits_{b \in {\rm B}} \sum_{i=1}^n L(y_i,{a_{k - 1}}\left( {{x_i}} \right) + b\left( {{x_i}} \right)) $ $ b_k(x) \approx \mathop {argmin }\limits_{b \in {\rm B}} \sum_{i=1}^n -s_i^k b\left( {{x_i}} \right)$
     - Обновление ансамбля: $ a_{k+1}(x) = a_{k}(x) + \eta \cdot b_k(x) $

3. **Критерий остановки**:
   - Достижение максимального числа базовых алгоритмов
   - Достижение заданой точности
   - Достигнутая точность не улучшается на протяжении шагов

---

### **Особенности и регуляризация**
- **Функции потерь**: Могут быть любыми дифференцируемыми (квадратичная, логистическая, Huber).
- **Регуляризация**:
  - **Коэффициент обучения** ($ \eta $): Уменьшает вклад каждой модели, предотвращая переобучение.
  - **Глубина деревьев**: Ограничение сложности базовых моделей.
  - **Субсэмплирование**: Обучение на случайных подвыборках данных или признаков (как в Stochastic Gradient Boosting).

    
[//]: # (deepseek)

---

**LightGBM** (Light Gradient Boosting Machine) — это открытый фреймворк для реализации алгоритма **градиентного бустинга**, разработанный компанией Microsoft. Он оптимизирован для высокой скорости обучения, эффективности работы с большими данными и низкого потребления памяти. 


[//]: # (https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)

### Основные особенности:

1. **GOSS (Gradient-based One-Side Sampling)**: Ускоряет обучение, отбирая для построения деревьев объекты с большими градиентами, игнорируя часть объектов с малыми градиентами.
2. **EFB (Exclusive Feature Bundling)**: Объединяет редко используемые признаки, уменьшая размерность данных без потери информации.
3. **Горизонтальный и вертикальный рост деревьев**: Деревья строятся по листьям, что снижает ошибку и ускоряет обучение.
4. **Работа с большими данными**: Эффективен на датасетах с миллионами строк и тысяч признаков.


### Преимущества:
- **Скорость**: Обучение быстрее, чем у XGBoost или CatBoost, особенно на больших данных
- **Точность**: При настройке гиперпараметров показывает один из лучших результатов среди алгоримов градиентного бустинга
- **Гибкость**: Поддерживает пользовательские функции потерь, метрики, а также работу с GPU
- **Интерпретируемость**: Возможность анализа важности признаков

### Недостатки:
- **Переобучение**: Может переобучаться на небольших датасетах, если не настроены гиперпараметры (например, `max_depth`, `num_leaves`).
- **Чувствительность к гиперпараметрам**: Требует тщательной настройки.

